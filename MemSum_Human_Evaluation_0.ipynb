{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/ReproHum_072904_DCU25/blob/main/MemSum_Human_Evaluation_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9F_RLOEXFXG"
      },
      "source": [
        "# MemSum Human Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone repos, Install Dependencies, Set working directory (takes about 4 minutes)\n",
        "\n"
      ],
      "metadata": {
        "id": "51qspy3Ml9Sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Drive, Set save and load folder (folder that contains the notebooks)\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "gdrive_folder_name = ''\n",
        "drive_folders = glob.glob('/content/drive/MyDrive/*')\n",
        "for drive_folder in drive_folders:\n",
        "  if drive_folder.startswith('/content/drive/MyDrive/2025_ReproHum'):\n",
        "    head, tail = os.path.split(drive_folder)\n",
        "    gdrive_folder_name = tail\n",
        "\n",
        "print(f'Annotation folder: {gdrive_folder_name}')"
      ],
      "metadata": {
        "id": "Dz1Sq4YaoGuj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clone MemSum, change working directory\n",
        "!git clone https://github.com/nianlonggu/MemSum.git\n",
        "\n",
        "import os\n",
        "import json\n",
        "os.chdir(\"MemSum\")\n",
        "\n",
        "#SM---START---\n",
        "def prepare_json():\n",
        "  human_eval_data_orig = [ json.loads(line) for line in open('human_eval_results/records_memsum_wo_autostop_neusum.jsonl',\"r\") ]\n",
        "  human_eval_data_orig_no_rankings = []\n",
        "  x = 0\n",
        "  while x < len(human_eval_data_orig):\n",
        "    new_dict = {}\n",
        "    for key in human_eval_data_orig[x].keys():\n",
        "      if key != 'ranking_results':\n",
        "        new_dict[key] = human_eval_data_orig[x][key]\n",
        "    human_eval_data_orig_no_rankings.append(new_dict)\n",
        "    x += 1\n",
        "  with open('human_eval_results/records_memsum_wo_autostop_neusum_clean.jsonl', 'w') as f:\n",
        "    for item in human_eval_data_orig_no_rankings:\n",
        "      f.write(json.dumps(item) + '\\n')\n",
        "  !rm /content/MemSum/human_eval_results/records_memsum_wo_autostop_neusum.jsonl\n",
        "  !rm /content/MemSum/human_eval_results/records_memsum_neusum.jsonl\n",
        "\n",
        "prepare_json()\n",
        "#SM---END---"
      ],
      "metadata": {
        "id": "GTyTvCtTBZFo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9c2dosErA8z-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Requirements\n",
        "!pip install -r requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clone Sent2Vec\n",
        "!git clone https://github.com/epfml/sent2vec/\n",
        "!cd sent2vec && git reset --hard 770bd2d && make && pip install ."
      ],
      "metadata": {
        "id": "Fgk32yWmC8oh",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Change working directory\n",
        "os.chdir(\"../MemSum\")"
      ],
      "metadata": {
        "id": "8xxgQwwNjdi5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Sent2Vec\n",
        "import sent2vec\n",
        "\n",
        "global sent2vec_model\n",
        "sent2vec_model = sent2vec.Sent2vecModel()\n",
        "sent2vec_model.load_model('/content/drive/MyDrive/'+gdrive_folder_name+'/wiki_unigrams.bin')"
      ],
      "metadata": {
        "id": "rg6qPWR6kySm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X4AVd4MSdC5"
      },
      "source": [
        "## Utils for evaluation interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_JT5abm29iz",
        "tags": [],
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Eval code\n",
        "import requests\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout, Button, Box, FloatText, Textarea, Dropdown, Label, IntSlider, GridspecLayout\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "import numpy as np\n",
        "import pprint\n",
        "import nltk\n",
        "from scipy.stats import ttest_rel , ttest_ind, wilcoxon\n",
        "from scipy.spatial.distance import cosine\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "#SM---START---\n",
        "from requests import get\n",
        "filename = get('http://172.28.0.12:9000/api/sessions').json()[0]['name']\n",
        "id_notebook = filename.rsplit('_', 1)[1].rsplit('.', 1)[0]\n",
        "\n",
        "load_partially_completed_file = False #@param{type:\"boolean\"}\n",
        "path_file_to_load = \"/content/drive/MyDrive/\"+gdrive_folder_name+\"/saved_annotations1(1)\"#+id_notebook\n",
        "# path_file_to_load = \"/content/drive/MyDrive/2025_ReproNLP_Annotators_DCU/saved_annotations\"+id_notebook\n",
        "if load_partially_completed_file == True:\n",
        "  if path_file_to_load == None:\n",
        "    print('ERROR: Please enter a filepath.')\n",
        "    sys.exit()\n",
        "  else:\n",
        "    if os.path.exists(path_file_to_load) == False:\n",
        "      print('ERROR: Filepath does not exist.')\n",
        "      sys.exit()\n",
        "    else:\n",
        "      print('Found file to load.')\n",
        "#SM---END---\n",
        "\n",
        "def get_summ_example():\n",
        "    global human_eval_data, current_doc_idx, num_of_eval_docs\n",
        "\n",
        "    found = False\n",
        "    for pos in range( current_doc_idx, len(human_eval_data) ):\n",
        "        #SM---START---\n",
        "        # Commented this block, which was probably there for showing the results only (it skips evaluation items)\n",
        "        # if human_eval_data[pos][\"ranking_results\"][\"overall\"] == [1,1]:\n",
        "        #     human_eval_data[pos][\"new_human_eval_results\"] = human_eval_data[pos][\"ranking_results\"]\n",
        "        #     num_of_eval_docs += 1\n",
        "        #     if num_of_eval_docs >= len(human_eval_data):\n",
        "        #         submit_button.disabled = True\n",
        "        # else:\n",
        "        #SM---END---\n",
        "        #SM---START---\n",
        "        # Unindented this block\n",
        "        found = True\n",
        "        current_doc_idx = pos\n",
        "        break\n",
        "        #SM---END---\n",
        "    if found:\n",
        "        summ_example = human_eval_data[current_doc_idx]\n",
        "        current_doc_idx = min(current_doc_idx+1, len(human_eval_data) )\n",
        "    else:\n",
        "        summ_example = None\n",
        "        current_doc_idx = len(human_eval_data)\n",
        "\n",
        "    return summ_example\n",
        "\n",
        "\n",
        "class TextHTML(widgets.HTML):\n",
        "    def __init__(self, html_style = {} ,**kwargs):\n",
        "        super().__init__(**kwargs )\n",
        "        self.default_html_style = {\n",
        "            \"padding\":\"5px\",\n",
        "            \"height\":\"600px\",\n",
        "            \"overflow-x\":\"hidden\",\n",
        "            \"border\":\"1px solid grey\",\n",
        "            \"line-height\":\"20px\"\n",
        "         }\n",
        "        self.render_sen_list(html_style=html_style)\n",
        "        self.html_lines = []\n",
        "\n",
        "    def render_sen_list(self, sens=[], html_style = {}):\n",
        "        self.default_html_style.update(html_style)\n",
        "        html_lines = [\n",
        "            '''<div style=\"%s\">''' %( \"; \".join( \":\".join([key, value]) for key, value in self.default_html_style.items() )  )\n",
        "        ]\n",
        "\n",
        "        for sen in sens:\n",
        "            is_marked = sen.get(\"is_marked\", False)\n",
        "            sen_text = sen.get(\"text\", \"\").capitalize()\n",
        "            html_line = '''<p> %s %s %s</p>'''%( '''<span style=\"background-color: #FFFF00\">''' if is_marked else \"\",\n",
        "                                             sen_text,\n",
        "                                             '''</span>''' if is_marked else \"\"\n",
        "                                           )\n",
        "            html_lines.append( html_line )\n",
        "\n",
        "        html_lines.append( \"</div>\" )\n",
        "        value = \"\\n\".join(html_lines)\n",
        "        self.value = value\n",
        "        self.html_lines = html_lines\n",
        "\n",
        "    def update_html_style( self, html_style = {} ):\n",
        "        self.default_html_style.update(html_style)\n",
        "        self.html_lines[0] = '''<div style=\"%s\">''' %( \"; \".join( \":\".join([key, value]) for key, value in self.default_html_style.items() )  )\n",
        "        value = \"\\n\".join(self.html_lines)\n",
        "        self.value = value\n",
        "\n",
        "\n",
        "def get_cosine_sim(query, sentences, model):\n",
        "  query_vec = model.embed_sentence(query)\n",
        "  sentences_vec = model.embed_sentences(sentences)\n",
        "  cosine_sim = [ 1-cosine(query_vec[0], sent_vec) for sent_vec in sentences_vec ]\n",
        "  return cosine_sim\n",
        "\n",
        "\n",
        "# Function to highlight text when the button is pressed\n",
        "def on_highlight_button_click(change):\n",
        "    # Update the HTML for each summary with highlighted text\n",
        "    global summ_example, query_text, sent2vec_model\n",
        "    query = query_text.value\n",
        "\n",
        "    # highlight query\n",
        "\n",
        "    summaryA = summ_example[\"random_extracted_results\"][0][0]\n",
        "    summaryA_cosine_sim = get_cosine_sim(query, summaryA, sent2vec_model)\n",
        "    list_sent = []\n",
        "    for idx, sent in enumerate(summaryA):\n",
        "      if summaryA_cosine_sim[idx]>0.6:\n",
        "        list_sent.append({\"text\":f'<mark>{sent}</mark>'})\n",
        "      else:\n",
        "        list_sent.append({\"text\":sent})\n",
        "    text_summ_sources[\"Summary A\"].render_sen_list( list_sent )\n",
        "\n",
        "    summaryB = summ_example[\"random_extracted_results\"][1][0]\n",
        "    summaryB_cosine_sim = get_cosine_sim(query, summaryB, sent2vec_model)\n",
        "    list_sent = []\n",
        "    for idx, sent in enumerate(summaryB):\n",
        "      if summaryB_cosine_sim[idx]>0.6:\n",
        "        list_sent.append({\"text\":f'<mark>{sent}</mark>'})\n",
        "      else:\n",
        "        list_sent.append({\"text\":sent})\n",
        "    text_summ_sources[\"Summary B\"].render_sen_list( list_sent )\n",
        "\n",
        "\n",
        "form_item_layout = Layout(\n",
        "    display='flex',\n",
        "    flex_flow='row',\n",
        "    justify_content='space-between'\n",
        ")\n",
        "\n",
        "rb_criteria = {}\n",
        "for criterion in [\"Overall:\"]:\n",
        "    rb_criteria[criterion] =  widgets.RadioButtons(\n",
        "                options=['summary A', 'summary B'],\n",
        "                disabled=False,\n",
        "                index = None\n",
        "    )\n",
        "\n",
        "b_summ_sources = {}\n",
        "colors_for_b_summ_sources ={ \"Reference Summary\":\"YellowGreen\",\"Summary A\":\"lightblue\", \"Summary B\":\"lightblue\" }\n",
        "for source in [\"Reference Summary\", \"Summary A\", \"Summary B\"]:\n",
        "    b_summ_sources[source] = Button(description=source, layout=Layout(height='auto', width='auto'))\n",
        "    b_summ_sources[source].style.button_color = colors_for_b_summ_sources[source]\n",
        "\n",
        "text_summ_sources = {}\n",
        "for source in [\"Reference Summary\", \"Summary A\", \"Summary B\"]:\n",
        "    text_summ_sources[source] =  TextHTML({\"height\":\"500px\"})  # Textarea(layout=Layout(height=\"600px\", width='auto'))\n",
        "\n",
        "submit_button = Button(description=\"Submit & Eval Next\", layout=Layout(height='auto', width='auto'))\n",
        "submit_button.style.button_color = \"LightSalmon\"\n",
        "\n",
        "\n",
        "global fulltext_textbox\n",
        "fulltext_button = Button(description=\"Show Source Document >>>\", layout=Layout(height='auto', width='32.9%'))\n",
        "fulltext_textbox = TextHTML( html_style={\"height\":\"0px\"}, layout=Layout(visibility=\"hidden\") )\n",
        "\n",
        "grid_b_summ = GridspecLayout(1,3)\n",
        "grid_b_summ[0,0] = b_summ_sources[\"Reference Summary\"]\n",
        "grid_b_summ[0,1] = b_summ_sources[\"Summary A\"]\n",
        "grid_b_summ[0,2] = b_summ_sources[\"Summary B\"]\n",
        "grid_text_summ = GridspecLayout(1,3)\n",
        "grid_text_summ[0,0] = text_summ_sources[\"Reference Summary\"]\n",
        "grid_text_summ[0,1] = text_summ_sources[\"Summary A\"]\n",
        "grid_text_summ[0,2] = text_summ_sources[\"Summary B\"]\n",
        "grid_rb_description = GridspecLayout(1,3)\n",
        "grid_rb_description[0,0] = Label(value = \"Overall:\")\n",
        "\n",
        "\n",
        "global query_text\n",
        "highlight_button = Button(description=\"Highlight relevant sentences given a query 🔍\", layout=Layout(height='auto', width='auto'))\n",
        "highlight_button.on_click(on_highlight_button_click)\n",
        "query_text = Textarea(placeholder='Enter your query here...', layout=Layout(width='auto', height='auto'))\n",
        "\n",
        "grid_query = GridspecLayout(1,3)\n",
        "grid_query[0,0] = highlight_button\n",
        "grid_query[0,1:] = query_text\n",
        "\n",
        "output_panel = widgets.Output()\n",
        "\n",
        "\n",
        "form_items = [\n",
        "    widgets.HTML(value = f\"<b><font color='black' font size='4pt'>Read</b>\"),\n",
        "    grid_query,\n",
        "    grid_b_summ,\n",
        "    grid_text_summ,\n",
        "    fulltext_button,\n",
        "    fulltext_textbox,\n",
        "    widgets.HTML(value = f\"<b><font color='black' font size='4pt'>Evaluation (choose one that is closer to the reference summary)</b>\"),\n",
        "    grid_rb_description,\n",
        "    widgets.HBox([ rb_criteria[\"Overall:\"] ], layout=form_item_layout),\n",
        "    widgets.Box([  submit_button ], layout=form_item_layout),\n",
        "    output_panel\n",
        "\n",
        "]\n",
        "\n",
        "gui = Box(form_items, layout=Layout(\n",
        "    display='flex',\n",
        "    flex_flow='column',\n",
        "    border='solid 2px',\n",
        "    align_items='stretch',\n",
        "    width='100%'\n",
        "))\n",
        "\n",
        "\n",
        "def get_next_example():\n",
        "    global summ_example, num_of_eval_docs, human_eval_data\n",
        "    summ_example = get_summ_example()\n",
        "    if summ_example is not None:\n",
        "        text_summ_sources[\"Reference Summary\"].render_sen_list( [{\"text\":_} for _ in summ_example[\"summary\"]] )\n",
        "        text_summ_sources[\"Summary A\"].render_sen_list( [{\"text\":_} for _ in summ_example[\"random_extracted_results\"][0][0] ] )\n",
        "        text_summ_sources[\"Summary B\"].render_sen_list( [{\"text\":_} for _ in summ_example[\"random_extracted_results\"][1][0] ] )\n",
        "    else:\n",
        "        text_summ_sources[\"Reference Summary\"].render_sen_list( [] )\n",
        "        text_summ_sources[\"Summary A\"].render_sen_list( [] )\n",
        "        text_summ_sources[\"Summary B\"].render_sen_list( [] )\n",
        "\n",
        "\n",
        "    for criterion in [\"Overall:\"]:\n",
        "        rb_criteria[criterion].index = None\n",
        "\n",
        "    if summ_example is not None:\n",
        "        fulltext_textbox.render_sen_list( [{\"text\":_} for _ in summ_example[\"text\"]] )\n",
        "    else:\n",
        "        fulltext_textbox.render_sen_list( [] )\n",
        "\n",
        "    fulltext_textbox.update_html_style({\"height\":\"0px\"})\n",
        "    fulltext_textbox.layout.visibility = \"hidden\"\n",
        "    fulltext_button.description = \"Show Source Document >>>\"\n",
        "\n",
        "    # empty search textbox\n",
        "    query_text.value = \"\"\n",
        "\n",
        "def fulltext_button_on_click_listener(_):\n",
        "    if fulltext_button.description == \"Show Source Document >>>\":\n",
        "        fulltext_textbox.update_html_style({\"height\":\"600px\"})\n",
        "        fulltext_textbox.layout.visibility = \"visible\"\n",
        "        fulltext_button.description = \"Hide Source Document >>>\"\n",
        "    elif fulltext_button.description == \"Hide Source Document >>>\":\n",
        "        fulltext_textbox.update_html_style({\"height\":\"0px\"})\n",
        "        fulltext_textbox.layout.visibility = \"hidden\"\n",
        "        fulltext_button.description = \"Show Source Document >>>\"\n",
        "fulltext_button.on_click( fulltext_button_on_click_listener )\n",
        "\n",
        "\n",
        "def submit_button_on_click_listener(_):\n",
        "    global summ_example, num_of_eval_docs\n",
        "    all_evaluated = True\n",
        "    for criterion in [\"Overall:\"]:\n",
        "        if rb_criteria[criterion].index is None:\n",
        "            with output_panel:\n",
        "                clear_output()\n",
        "                print(\"You have not evaluated %s, please retry.\"%( criterion.rstrip(\":\") ))\n",
        "            all_evaluated = False\n",
        "    if all_evaluated:\n",
        "        two_orders = [ [1,2],[2,1] ]\n",
        "        summ_example[\"new_human_eval_results\"] = {\n",
        "                                         \"overall\":two_orders[ rb_criteria[\"Overall:\"].index ]\n",
        "                                    }\n",
        "        num_of_eval_docs += 1\n",
        "        if num_of_eval_docs >= len(human_eval_data):\n",
        "            submit_button.disabled = True\n",
        "        else:\n",
        "            get_next_example()\n",
        "        with output_panel:\n",
        "            clear_output()\n",
        "            print(\"You have evaluated %d/%d examples.\"%( num_of_eval_docs, len(human_eval_data)))\n",
        "\n",
        "    #SM---START---\n",
        "    # print(num_of_eval_docs, current_doc_idx)\n",
        "    # print(human_eval_data[num_of_eval_docs-1][\"new_human_eval_results\"])\n",
        "    with open(path_file_to_load, 'wb') as f:\n",
        "        pickle.dump(human_eval_data, f)\n",
        "    #SM---END---\n",
        "\n",
        "submit_button.on_click( submit_button_on_click_listener )\n",
        "\n",
        "human_eval_data = None\n",
        "summ_example = None\n",
        "num_of_eval_docs = 0\n",
        "current_doc_idx = 0\n",
        "\n",
        "def run_gui( dataset_path,  width =\"90%\", textbox_height = \"400px\", ):\n",
        "    global human_eval_data, summ_example, num_of_eval_docs, current_doc_idx\n",
        "    #SM---START---\n",
        "    num_annnotations_loaded = 0\n",
        "    if load_partially_completed_file == True:\n",
        "        if load_partially_completed_file == True:\n",
        "            with open(path_file_to_load, 'rb') as f:\n",
        "                human_eval_data = pickle.load(f)\n",
        "                for z in range(len(human_eval_data)):\n",
        "                    if 'new_human_eval_results' in human_eval_data[z].keys():\n",
        "                        num_annnotations_loaded += 1\n",
        "    else:\n",
        "    #SM---END---\n",
        "        human_eval_data = [ json.loads(line) for line in open(dataset_path,\"r\") ]\n",
        "\n",
        "    summ_example = None\n",
        "    num_of_eval_docs = 0\n",
        "    current_doc_idx = 0\n",
        "    #SM---START---\n",
        "    if load_partially_completed_file == True:\n",
        "        num_of_eval_docs = num_annnotations_loaded\n",
        "        current_doc_idx = num_annnotations_loaded\n",
        "    #SM---END---\n",
        "    get_next_example()\n",
        "    with output_panel:\n",
        "        clear_output()\n",
        "        print(\"You have evaluated %d/%d examples.\"%( num_of_eval_docs, len(human_eval_data)))\n",
        "\n",
        "    for source in [\"Reference Summary\", \"Summary A\", \"Summary B\"]:\n",
        "        text_summ_sources[source].update_html_style({\"height\":textbox_height})\n",
        "    gui.layout.width = width\n",
        "    return gui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcoOZ8PrTS57"
      },
      "source": [
        "## Human Evaluation Experiment II:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtWAf2MF3B8n",
        "tags": []
      },
      "outputs": [],
      "source": [
        "run_gui(dataset_path = \"human_eval_results/records_memsum_wo_autostop_neusum_clean.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug (for team carrying out the reproduction)"
      ],
      "metadata": {
        "id": "uwobfQqjeHP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_eval_data[0]"
      ],
      "metadata": {
        "id": "2jfNyczaumsE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Control loaded file\n",
        "with open(path_file_to_load, 'rb') as f:\n",
        "  human_eval_data_load = pickle.load(f)\n",
        "  for z in range(len(human_eval_data_load)):\n",
        "    print(z)\n",
        "    if 'new_human_eval_results' in human_eval_data_load[z].keys():\n",
        "      print(human_eval_data_load[z]['new_human_eval_results'])"
      ],
      "metadata": {
        "id": "CuhvoRHkWsrD",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Control human_eval_data list: new annotations\n",
        "print(len(human_eval_data), 'datapoints.')\n",
        "print(human_eval_data[0].keys())\n",
        "print()\n",
        "\n",
        "x = 0\n",
        "while x < num_of_eval_docs:\n",
        "  print(human_eval_data[x]['new_human_eval_results'])\n",
        "  x += 1"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B1AYXmSxp6TN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Control human_eval_data list: existing data\n",
        "human_eval_data_orig = [ json.loads(line) for line in open('human_eval_results/records_memsum_wo_autostop_neusum_clean.jsonl',\"r\") ]\n",
        "\n",
        "print(len(human_eval_data_orig), 'datapoints.')\n",
        "print(human_eval_data_orig[0].keys())\n",
        "print()\n",
        "\n",
        "x = 0\n",
        "while x < len(human_eval_data_orig):\n",
        "  print(str(x), human_eval_data_orig[x]['random_orders'], human_eval_data_orig[x]['unique_id'])\n",
        "  # print(human_eval_data_orig[x].keys())\n",
        "  x += 1"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "LAQ8rnXw_f_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print examples\n",
        "print(summ_example['indice'])\n",
        "print(summ_example['summary'])\n",
        "print('----------------------')\n",
        "summ_example['random_extracted_results']"
      ],
      "metadata": {
        "id": "mHeJxJ4UHUFT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process results (for team carrying out the reproduction)"
      ],
      "metadata": {
        "id": "umSlbhdNNh17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set directory\n",
        "ReproNLP_dir = '/content/drive/MyDrive/2025_ReproNLP_Annotators_DCU/evals_results'"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XgofrOpoNrHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load annotated files and get averages\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "folders_results  = sorted(glob.glob(ReproNLP_dir + '/*'))\n",
        "\n",
        "def scores2df(path_file_to_load, count_items, df_scores, newfile = True):\n",
        "  first_item_id = count_items\n",
        "  if newfile == True:\n",
        "    # In case of \"regular\" files (i.e. one file containing all items), start numbering from 0. In case of items distributed in 2 files, keep counting from the end of the previous file\n",
        "    first_item_id = 0\n",
        "  with open(path_file_to_load, 'rb') as f:\n",
        "    # Load file with scores\n",
        "    human_eval_data_load = None\n",
        "    annotator_id = None\n",
        "    # That's the key of the dico hwhere the results can be found (different names in original and new files)\n",
        "    rankings_key = None\n",
        "    if '.jsonl' in path_file_to_load:\n",
        "      # That's for loading the original experiment json file\n",
        "      human_eval_data_load = [ json.loads(line) for line in open(path_file_to_load,\"r\") ]\n",
        "      rankings_key = 'ranking_results'\n",
        "      # print(human_eval_data_load)\n",
        "    else:\n",
        "      # That's for loading our new evaluation files collected during the reproduction\n",
        "      # Get annotator ID from path\n",
        "      annotator_id = os.path.split(path_file_to_load)[1].split('saved_annotations')[1][0]\n",
        "      human_eval_data_load = pickle.load(f)\n",
        "      rankings_key = 'new_human_eval_results'\n",
        "    # Get scores from the first item to the last (annotator 1 has their scores in 2 files in the reproduction, the first item if the second file is item 35)\n",
        "    for z in range(first_item_id, len(human_eval_data_load)):\n",
        "      # print(human_eval_data_load[z].keys())\n",
        "      if rankings_key in human_eval_data_load[z].keys():\n",
        "        # print(count_items)\n",
        "        # Get rankings of systems (list [1, 2] or [2, 1])\n",
        "        ranks = human_eval_data_load[z][rankings_key]['overall']\n",
        "        assert len(ranks) == 2, f'There should be 2 values in the ranks list, found {len(ranks)}.'\n",
        "        # The next one is deactivated because in the original data there are ties [1, 1]\n",
        "        # assert ranks[0] != ranks[1], f'There should be 2 different values in the ranks list, found {ranks[0]} and {ranks[1]}.'\n",
        "        assert ranks[0] == 1 or ranks[0] == 2, f'The first value in the ranks list should be 1 or 2, found {ranks[0]}.'\n",
        "        assert ranks[1] == 1 or ranks[1] == 2, f'The second value in the ranks list should be 1 or 2, found {ranks[1]}.'\n",
        "        # Get the system order (['MemSum', 'NeuSum'] or ['NeuSum', 'MemSum'])\n",
        "        systems = human_eval_data_load[z]['random_orders']\n",
        "        # Get the position of each system in the system order list\n",
        "        memsum_pos = systems.index('MemSum')\n",
        "        neusum_pos = systems.index('NeuSum')\n",
        "        # print(ranks, systems)\n",
        "        # print(f'memsum: {ranks[memsum_pos]}')\n",
        "        # print(f'neusum: {ranks[neusum_pos]}')\n",
        "        # Get the rank of each system by matching the position in the system list and the position in the rank list\n",
        "        df_scores.loc[count_items] = [annotator_id, z, ranks[memsum_pos], ranks[neusum_pos]]\n",
        "        count_items += 1\n",
        "  return(df_scores, count_items)\n",
        "\n",
        "def get_mean_scores(df_scores, count_files):\n",
        "  # Average scores per annotator\n",
        "  memsum_avg_by_annot = round(df_scores.groupby(['id_annot']).agg(MemSum=('MemSum', 'mean')), 2)\n",
        "  neusum_avg_by_annot = round(df_scores.groupby(['id_annot']).agg(NeuSum=('NeuSum', 'mean')), 2)\n",
        "  # Average scores across annotators\n",
        "  memsum_avg = round(df_scores['MemSum'].mean(), 2)\n",
        "  neusum_avg = round(df_scores['NeuSum'].mean(), 2)\n",
        "  print()\n",
        "  print(memsum_avg_by_annot)\n",
        "  print()\n",
        "  print(neusum_avg_by_annot)\n",
        "  print()\n",
        "  print(f'MemSum average ({count_files} evaluators): {memsum_avg}')\n",
        "  print(f'NeuSum average ({count_files} evaluators): {neusum_avg}')\n",
        "  return memsum_avg, neusum_avg\n",
        "\n",
        "def aggregate_scores_per_item(df_scores):\n",
        "  # print(df_scores.groupby(['id_item']).agg(MemSum=('MemSum', 'mean')))\n",
        "  # print(df_scores.groupby(['id_item']).agg(NeuSum=('NeuSum', 'mean')))\n",
        "  df_scores_per_item = (df_scores.groupby(['id_item']).sum())\n",
        "  for index, row in df_scores_per_item.iterrows():\n",
        "    if row['MemSum'] < row['NeuSum']:\n",
        "      # Add a column to store the aggregated score for each system (1 or 2)\n",
        "      df_scores_per_item.loc[index, 'MemSum'] = 1\n",
        "      df_scores_per_item.loc[index, 'NeuSum'] = 2\n",
        "    elif row['MemSum'] > row['NeuSum']:\n",
        "      # Add a column to store the aggregated score for each system (1 or 2)\n",
        "      df_scores_per_item.loc[index, 'MemSum'] = 2\n",
        "      df_scores_per_item.loc[index, 'NeuSum'] = 1\n",
        "    else:\n",
        "      # Add a column to store the aggregated score for each system (1 or 2)\n",
        "      df_scores_per_item.loc[index, 'MemSum'] = 1\n",
        "      df_scores_per_item.loc[index, 'NeuSum'] = 1\n",
        "  display(df_scores_per_item)\n",
        "  return df_scores_per_item\n",
        "\n",
        "# Make dataframe headers\n",
        "df_scores = pd.DataFrame(columns=['id_annot', 'id_item', 'MemSum', 'NeuSum'])\n",
        "count_items = 0\n",
        "count_files = 0\n",
        "for folder_results in folders_results:\n",
        "  id_notebook = os.path.basename(folder_results)\n",
        "  path_file_to_load = os.path.join(folder_results, 'saved_annotations'+str(id_notebook))\n",
        "  if os.path.exists(path_file_to_load):\n",
        "    print(f'Processing {path_file_to_load}...')\n",
        "    # If we have a fully annotated file (original annnotators 2, 3 and 4), simply use the file\n",
        "    df_scores, count_items = scores2df(path_file_to_load, count_items, df_scores)\n",
        "    print(f'  {count_items} total rankings collected...')\n",
        "    count_files += 1\n",
        "  else:\n",
        "    # For one annotator, we have to combine scores in 2 files\n",
        "    if 'saved_annotations1' in path_file_to_load:\n",
        "      path_file_to_load1 = os.path.join(ReproNLP_dir, '1', 'saved_annotations1_first35')\n",
        "      path_file_to_load2 = os.path.join(ReproNLP_dir, '1', 'saved_annotations1_last28')\n",
        "      print(f'Processing /content/drive/MyDrive/2025_ReproNLP_Annotators_DCU/evals_results/2/saved_annotations1_1-35...')\n",
        "      df_scores, count_items = scores2df(path_file_to_load1, count_items, df_scores)\n",
        "      print(f'  {count_items} total rankings collected...')\n",
        "      print(f'Processing /content/drive/MyDrive/2025_ReproNLP_Annotators_DCU/evals_results/2/saved_annotations1_36-63...')\n",
        "      # newfile == True (True is the default value in the function args) means that the first item counter is reinitialised (so the 63 rows for each annotator are numbered from 0 to 62)\n",
        "      df_scores, count_items = scores2df(path_file_to_load2, count_items, df_scores, newfile = False)\n",
        "      print(f'  {count_items} total rankings collected...')\n",
        "      count_files += 1\n",
        "\n",
        "assert count_items/count_files == 63, f'Error, there should be 63 items per annotator, fount {count_items/count_files}.'\n",
        "print('63 ratings per annotator were found.')\n",
        "print()\n",
        "print(df_scores)\n",
        "\n",
        "# Compute and print average scores over all 252 ratings\n",
        "memsum_avg_repro, neusum_avg_repro = get_mean_scores(df_scores, count_files)\n",
        "\n",
        "# Aggregate scores per evalaution item (looks like that's what was done in the original paper)\n",
        "df_scores_per_item = aggregate_scores_per_item(df_scores)\n",
        "# Compute and print average scores over all 63 aggregated ratings\n",
        "memsum_avg_repro_agg, neusum_avg_repro_agg = get_mean_scores(df_scores_per_item, count_files)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "RBRaUaK_NnzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Process the original results file and get averages\n",
        "\n",
        "assert not os.path.exists('/content/MemSum/human_eval_results/records_memsum_wo_autostop_neusum_clean.jsonl'), 'You need to delete and restart the runtime NOT RUN the \"Clone MemSum, change working directory\" cell above!'\n",
        "if not os.path.exists('/content/MemSum'):\n",
        "  !git clone https://github.com/nianlonggu/MemSum.git\n",
        "\n",
        "dataset_path = '/content/MemSum/human_eval_results/records_memsum_wo_autostop_neusum.jsonl'\n",
        "\n",
        "df_scores_new = pd.DataFrame(columns=['id_annot', 'id_item', 'MemSum', 'NeuSum'])\n",
        "count_items = 0\n",
        "df_scores_orig, count_items = scores2df(dataset_path, count_items, df_scores_new)\n",
        "count_files = 1\n",
        "print(f'  {count_items} total rankings collected...')\n",
        "\n",
        "memsum_avg_orig, neusum_avg_orig = get_mean_scores(df_scores_orig, count_files)\n",
        "print()\n",
        "print('Results in paper: MemSum=1.38, NeuSum=1.57')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gfxHSEDsOeKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make CSV file for running QRA+ code\n",
        "\n",
        "# Make CSV for scores not averaged\n",
        "df_csv_qra = pd.DataFrame(columns=['Key', 'Paper', 'Study', 'System', 'Criterion', 'Result'])\n",
        "df_csv_qra.loc[0] = ['0729-04', 'Gu et al 2022', 'Original', 'MemSum', 'Overall', memsum_avg_orig]\n",
        "df_csv_qra.loc[1] = ['0729-04', 'Gu et al 2022', 'Original', 'NeuSum', 'Overall', neusum_avg_orig]\n",
        "df_csv_qra.loc[2] = ['0729-04', 'Gu et al 2022', 'Reproduction 1', 'MemSum', 'Overall', memsum_avg_repro]\n",
        "df_csv_qra.loc[3] = ['0729-04', 'Gu et al 2022', 'Reproduction 1', 'NeuSum', 'Overall', neusum_avg_repro]\n",
        "\n",
        "# Save df_csv_qra in csv file\n",
        "df_csv_qra.to_csv('/content/gu-etal-2022_DCU.csv', index=False)\n",
        "print('Created /content/gu-etal-2022_DCU.csv')\n",
        "\n",
        "# Make CSV for item-level averaged scores\n",
        "df_csv_qra_agg = pd.DataFrame(columns=['Key', 'Paper', 'Study', 'System', 'Criterion', 'Result'])\n",
        "df_csv_qra_agg.loc[0] = ['0729-04', 'Gu et al 2022', 'Original', 'MemSum', 'Overall', memsum_avg_orig]\n",
        "df_csv_qra_agg.loc[1] = ['0729-04', 'Gu et al 2022', 'Original', 'NeuSum', 'Overall', neusum_avg_orig]\n",
        "df_csv_qra_agg.loc[2] = ['0729-04', 'Gu et al 2022', 'Reproduction 1', 'MemSum', 'Overall', memsum_avg_repro_agg]\n",
        "df_csv_qra_agg.loc[3] = ['0729-04', 'Gu et al 2022', 'Reproduction 1', 'NeuSum', 'Overall', neusum_avg_repro_agg]\n",
        "\n",
        "# Save df_csv_qra in csv file\n",
        "df_csv_qra_agg.to_csv('/content/gu-etal-2022_DCU_agg.csv', index=False)\n",
        "print('Created /content/gu-etal-2022_DCU_agg.csv')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AUV-KOALaVnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate Fleiss's Kappa\n",
        "\n",
        "'''\n",
        "Created on Aug 1, 2016\n",
        "@author: skarumbaiah\n",
        "\n",
        "Computes Fleiss' Kappa\n",
        "Joseph L. Fleiss, Measuring Nominal Scale Agreement Among Many Raters, 1971.\n",
        "'''\n",
        "\n",
        "def checkInput(rate, n):\n",
        "    \"\"\"\n",
        "    Check correctness of the input matrix\n",
        "    @param rate - ratings matrix\n",
        "    @return n - number of raters\n",
        "    @throws AssertionError\n",
        "    \"\"\"\n",
        "    N = len(rate)\n",
        "    k = len(rate[0])\n",
        "    assert all(len(rate[i]) == k for i in range(k)), \"Row length != #categories)\"\n",
        "    assert all(isinstance(rate[i][j], int) for i in range(N) for j in range(k)), \"Element not integer\"\n",
        "    assert all(sum(row) == n for row in rate), \"Sum of ratings != #raters)\"\n",
        "\n",
        "def fleissKappa(rate,n):\n",
        "    \"\"\"\n",
        "    Computes the Kappa value\n",
        "    @param rate - ratings matrix containing number of ratings for each subject per category\n",
        "    [size - N X k where N = #subjects and k = #categories]\n",
        "    @param n - number of raters\n",
        "    @return fleiss' kappa\n",
        "    \"\"\"\n",
        "\n",
        "    N = len(rate)\n",
        "    k = len(rate[0])\n",
        "    print(\"#raters = \", n, \", #subjects = \", N, \", #categories = \", k)\n",
        "    checkInput(rate, n)\n",
        "\n",
        "    #mean of the extent to which raters agree for the ith subject\n",
        "    PA = sum([(sum([i**2 for i in row])- n) / (n * (n - 1)) for row in rate])/N\n",
        "    print(\"PA = \", PA)\n",
        "\n",
        "    # mean of squares of proportion of all assignments which were to jth category\n",
        "    PE = sum([j**2 for j in [sum([rows[i] for rows in rate])/(N*n) for i in range(k)]])\n",
        "    print(\"PE =\", PE)\n",
        "\n",
        "    kappa = -float(\"inf\")\n",
        "    try:\n",
        "        kappa = (PA - PE) / (1 - PE)\n",
        "        kappa = float(\"{:.3f}\".format(kappa))\n",
        "    except ZeroDivisionError:\n",
        "        print(\"Expected agreement = 1\")\n",
        "\n",
        "    print(\"Fleiss' Kappa =\", kappa)\n",
        "\n",
        "    return kappa\n",
        "\n",
        "# Code above expects a matrix of d rows and s columns where:\n",
        "# d is the number of data points (also called \"subjects\")\n",
        "# s is the number of categories, i.e. possible scores (in our case, for one system, that's 2: 1 and 2).\n",
        "# Then at each position in the matrix, there should be the number n of annotators who chose the score s for datapoint d.\n",
        "# rate = [\n",
        "#     [2,0,0,0],\n",
        "#     [0,2,0,0],\n",
        "#     [0,0,2,0],\n",
        "#     [0,0,0,2]\n",
        "# ]\n",
        "# kappa = fleissKappa(rate,2)\n",
        "# assert(kappa==1)\n",
        "\n",
        "def makeFleissMatrix(df_scores):\n",
        "  # get list with scores of each annotator for each input_id in df\n",
        "  scores_matrix = []\n",
        "  # Group rows per annotation item\n",
        "  df_scores_per_item = df_scores.groupby(['id_item'])\n",
        "  for key, item in df_scores_per_item:\n",
        "    # print(key[0])\n",
        "    count_memsum_first = 0\n",
        "    count_neusum_first = 0\n",
        "    # print(df_scores_per_item.get_group(key))\n",
        "    # Get the MemSum score for each annotator\n",
        "    for row in df_scores_per_item.get_group(key).iterrows():\n",
        "      if row[1]['MemSum'] == 1:\n",
        "        count_memsum_first += 1\n",
        "      elif row[1]['NeuSum'] == 1:\n",
        "        count_neusum_first += 1\n",
        "    assert count_memsum_first + count_neusum_first == 4, f'There should be 4 first ranks, found {count_memsum_first + count_neusum_first}.'\n",
        "    # print(f'  MemSum 1st/2nd count: {count_memsum_first}-{count_neusum_first}\\n')\n",
        "    # Put counts in the matrix\n",
        "    scores_matrix.append([count_memsum_first, count_neusum_first])\n",
        "  return scores_matrix\n",
        "\n",
        "# Prepare matrix for fleiss\n",
        "scores_matrix = makeFleissMatrix(df_scores)\n",
        "print(scores_matrix)\n",
        "\n",
        "# Calculate fleiss kappa\n",
        "kappa = fleissKappa(scores_matrix,4)\n",
        "\n",
        "# scores_matrix_test1 = [[4, 0], [3, 1], [0, 4], [4, 0], [1, 3], [4, 0], [0, 4], [3, 1], [3, 1], [1, 3], [4, 0], [0, 4], [1, 3], [3, 1], [3, 1], [3, 1], [4, 0], [4, 0], [3, 1], [1, 3], [1, 3], [4, 0], [3, 1], [4, 0], [4, 0], [0, 4], [0, 4], [4, 0], [4, 0], [4, 0], [4, 0], [4, 0], [3, 1], [1, 3], [1, 3], [3, 1], [0, 4], [4, 0], [0, 4], [3, 1], [1, 3], [0, 4], [1, 3], [0, 4], [3, 1], [0, 4], [1, 3], [0, 4], [3, 1], [4, 0], [4, 0], [1, 3], [4, 0], [0, 4], [0, 4], [1, 3], [1, 3], [0, 4], [0, 4], [1, 3], [3, 1], [3, 1], [0, 4]]\n",
        "# kappa = fleissKappa(scores_matrix_test1,4)\n",
        "# scores_matrix_test2 = [[4, 0], [4, 0], [0, 4], [4, 0], [0, 4], [4, 0], [0, 4], [4, 0], [4, 0], [0, 4], [4, 0], [0, 4], [0, 4], [4, 0], [4, 0], [4, 0], [4, 0], [4, 0], [4, 0], [0, 4], [0, 4], [4, 0], [4, 0], [4, 0], [4, 0], [0, 4], [0, 4], [4, 0], [4, 0], [4, 0], [4, 0], [4, 0], [4, 0], [0, 4], [0, 4], [4, 0], [0, 4], [4, 0], [0, 4], [4, 0], [0, 4], [0, 4], [0, 4], [0, 4], [4, 0], [0, 4], [0, 4], [0, 4], [4, 0], [4, 0], [4, 0], [0, 4], [4, 0], [0, 4], [0, 4], [0, 4], [0, 4], [0, 4], [0, 4], [0, 4], [4, 0], [4, 0], [0, 4]]\n",
        "# kappa = fleissKappa(scores_matrix_test2,4)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "O9gbpDZ4UZgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate Wilcoxon signed-rank test\n",
        "import scipy.stats as stats\n",
        "\n",
        "data_to_use = 'original_agg'#@param['original_agg', 'repro_all', 'repro_agg']\n",
        "\n",
        "scores_memsum_wilcox = None\n",
        "scores_neusum_wilcox = None\n",
        "if data_to_use == 'original_agg':\n",
        "  print('Processing original aggregated results file...')\n",
        "  scores_memsum_orig = df_scores_orig['MemSum'].tolist()\n",
        "  scores_neusum_orig = df_scores_orig['NeuSum'].tolist()\n",
        "  scores_memsum_wilcox = scores_memsum_orig\n",
        "  scores_neusum_wilcox = scores_neusum_orig\n",
        "elif data_to_use == 'repro_all':\n",
        "  print('Processing reproduction results file...')\n",
        "  scores_memsum_repro = df_scores['MemSum'].tolist()\n",
        "  scores_neusum_repro = df_scores['NeuSum'].tolist()\n",
        "  scores_memsum_wilcox = scores_memsum_repro\n",
        "  scores_neusum_wilcox = scores_neusum_repro\n",
        "elif data_to_use == 'repro_agg':\n",
        "  print('Processing reproduction aggregated results file...')\n",
        "  scores_memsum_repro_agg = df_scores_per_item['MemSum'].tolist()\n",
        "  scores_neusum_repro_agg = df_scores_per_item['NeuSum'].tolist()\n",
        "  scores_memsum_wilcox = scores_memsum_repro_agg\n",
        "  scores_neusum_wilcox = scores_neusum_repro_agg\n",
        "\n",
        "stats.wilcoxon(scores_memsum_wilcox, scores_neusum_wilcox)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B2jCPMVZnWfb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "51qspy3Ml9Sd",
        "uwobfQqjeHP1",
        "umSlbhdNNh17"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:memsum]",
      "language": "python",
      "name": "conda-env-memsum-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}